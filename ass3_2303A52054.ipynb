{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8YYpDVLo6tujNKwAumEpg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52054/HPC/blob/main/ass3_2303A52054.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T62bleSlDETB",
        "outputId": "df027185-5084-4b1d-c18a-6bbf32106974"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vector Size\tSerial Time(s)\tParallel Time(s)\tSpeedup\n",
            "-----------------------------------------------------------------\n",
            "  1,000,000\t0.0029\t\t0.0065\t\t\t0.45x\n",
            "  5,000,000\t0.0322\t\t0.0282\t\t\t1.14x\n",
            " 10,000,000\t0.0697\t\t0.0748\t\t\t0.93x\n",
            " 50,000,000\t0.4359\t\t0.3648\t\t\t1.19x\n",
            "100,000,000\t0.6693\t\t0.5516\t\t\t1.21x\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "from numba import njit, prange\n",
        "import time\n",
        "\n",
        "# Task 1: Serial Python function\n",
        "@njit\n",
        "def vector_saxpy_serial(alpha, A, B, C):\n",
        "    \"\"\"Serial implementation: C[i] = alpha * A[i] + B[i]\"\"\"\n",
        "    n = len(A)\n",
        "    for i in range(n):\n",
        "        C[i] = alpha * A[i] + B[i]\n",
        "    return C\n",
        "\n",
        "# Task 2: Parallel version using prange\n",
        "@njit(parallel=True)\n",
        "def vector_saxpy_parallel(alpha, A, B, C):\n",
        "    \"\"\"Parallel implementation using prange\"\"\"\n",
        "    n = len(A)\n",
        "    for i in prange(n):\n",
        "        C[i] = alpha * A[i] + B[i]\n",
        "    return C\n",
        "\n",
        "# Task 3 & 4: Measure runtime and compare performance\n",
        "def benchmark_vector_operations():\n",
        "    \"\"\"Measure runtime for increasing vector sizes\"\"\"\n",
        "    sizes = [1_000_000, 5_000_000, 10_000_000, 50_000_000, 100_000_000]\n",
        "    alpha = 2.5\n",
        "\n",
        "    print(\"Vector Size\\tSerial Time(s)\\tParallel Time(s)\\tSpeedup\")\n",
        "    print(\"-\" * 65)\n",
        "\n",
        "    for size in sizes:\n",
        "        A = np.random.rand(size).astype(np.float64)\n",
        "        B = np.random.rand(size).astype(np.float64)\n",
        "        C_serial = np.zeros(size, dtype=np.float64)\n",
        "        C_parallel = np.zeros(size, dtype=np.float64)\n",
        "\n",
        "        # Warm-up\n",
        "        vector_saxpy_serial(alpha, A[:1000], B[:1000], C_serial[:1000])\n",
        "        vector_saxpy_parallel(alpha, A[:1000], B[:1000], C_parallel[:1000])\n",
        "\n",
        "        # Serial timing\n",
        "        start = time.perf_counter()\n",
        "        vector_saxpy_serial(alpha, A, B, C_serial)\n",
        "        serial_time = time.perf_counter() - start\n",
        "\n",
        "        # Parallel timing\n",
        "        start = time.perf_counter()\n",
        "        vector_saxpy_parallel(alpha, A, B, C_parallel)\n",
        "        parallel_time = time.perf_counter() - start\n",
        "\n",
        "        speedup = serial_time / parallel_time\n",
        "\n",
        "        print(f\"{size:>11,}\\t{serial_time:.4f}\\t\\t{parallel_time:.4f}\\t\\t\\t{speedup:.2f}x\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_vector_operations()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "from numba import njit, prange\n",
        "import time\n",
        "\n",
        "# Task 1: Serial matrix multiplication\n",
        "@njit\n",
        "def matrix_multiply_serial(A, B, C):\n",
        "    \"\"\"Serial matrix multiplication\"\"\"\n",
        "    n = A.shape[0]\n",
        "    m = B.shape[1]\n",
        "    k = A.shape[1]\n",
        "\n",
        "    for i in range(n):\n",
        "        for j in range(m):\n",
        "            temp = 0.0\n",
        "            for p in range(k):\n",
        "                temp += A[i, p] * B[p, j]\n",
        "            C[i, j] = temp\n",
        "    return C\n",
        "\n",
        "# Task 2: Parallelize outer loop\n",
        "@njit(parallel=True)\n",
        "def matrix_multiply_parallel_outer(A, B, C):\n",
        "    \"\"\"Parallel matrix multiplication - outer loop parallelized\"\"\"\n",
        "    n = A.shape[0]\n",
        "    m = B.shape[1]\n",
        "    k = A.shape[1]\n",
        "\n",
        "    for i in prange(n):\n",
        "        for j in range(m):\n",
        "            temp = 0.0\n",
        "            for p in range(k):\n",
        "                temp += A[i, p] * B[p, j]\n",
        "            C[i, j] = temp\n",
        "    return C\n",
        "\n",
        "# Task 3: Parallelize using collapsed loops logic\n",
        "@njit(parallel=True)\n",
        "def matrix_multiply_parallel_collapsed(A, B, C):\n",
        "    \"\"\"Parallel matrix multiplication - collapsed loop approach\"\"\"\n",
        "    n = A.shape[0]\n",
        "    m = B.shape[1]\n",
        "    k = A.shape[1]\n",
        "\n",
        "    for i in prange(n * m):\n",
        "        row = i // m\n",
        "        col = i % m\n",
        "        temp = 0.0\n",
        "        for p in range(k):\n",
        "            temp += A[row, p] * B[p, col]\n",
        "        C[row, col] = temp\n",
        "    return C\n",
        "\n",
        "# Task 4: Analyze cache behavior and performance\n",
        "def benchmark_matrix_multiplication():\n",
        "    \"\"\"Compare serial vs parallel performance\"\"\"\n",
        "    sizes = [128, 256, 512, 1024]\n",
        "\n",
        "    print(\"Matrix Size\\tSerial(s)\\tParallel Outer(s)\\tCollapsed(s)\\tSpeedup(Outer)\\tSpeedup(Collapsed)\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for size in sizes:\n",
        "        A = np.random.rand(size, size).astype(np.float64)\n",
        "        B = np.random.rand(size, size).astype(np.float64)\n",
        "        C_serial = np.zeros((size, size), dtype=np.float64)\n",
        "        C_parallel_outer = np.zeros((size, size), dtype=np.float64)\n",
        "        C_parallel_collapsed = np.zeros((size, size), dtype=np.float64)\n",
        "\n",
        "        # Warm-up\n",
        "        matrix_multiply_serial(A[:10, :10], B[:10, :10], C_serial[:10, :10])\n",
        "        matrix_multiply_parallel_outer(A[:10, :10], B[:10, :10], C_parallel_outer[:10, :10])\n",
        "        matrix_multiply_parallel_collapsed(A[:10, :10], B[:10, :10], C_parallel_collapsed[:10, :10])\n",
        "\n",
        "        # Serial timing\n",
        "        start = time.perf_counter()\n",
        "        matrix_multiply_serial(A, B, C_serial)\n",
        "        serial_time = time.perf_counter() - start\n",
        "\n",
        "        # Parallel outer timing\n",
        "        start = time.perf_counter()\n",
        "        matrix_multiply_parallel_outer(A, B, C_parallel_outer)\n",
        "        parallel_outer_time = time.perf_counter() - start\n",
        "\n",
        "        # Parallel collapsed timing\n",
        "        start = time.perf_counter()\n",
        "        matrix_multiply_parallel_collapsed(A, B, C_parallel_collapsed)\n",
        "        parallel_collapsed_time = time.perf_counter() - start\n",
        "\n",
        "        speedup_outer = serial_time / parallel_outer_time\n",
        "        speedup_collapsed = serial_time / parallel_collapsed_time\n",
        "\n",
        "        print(f\"{size}x{size}\\t\\t{serial_time:.4f}\\t\\t{parallel_outer_time:.4f}\\t\\t\\t{parallel_collapsed_time:.4f}\\t\\t{speedup_outer:.2f}x\\t\\t{speedup_collapsed:.2f}x\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_matrix_multiplication()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOHUn5a7DGBl",
        "outputId": "13c5cc3c-be6e-4db9-ec9e-373ddacf016e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Matrix Size\tSerial(s)\tParallel Outer(s)\tCollapsed(s)\tSpeedup(Outer)\tSpeedup(Collapsed)\n",
            "----------------------------------------------------------------------------------------------------\n",
            "128x128\t\t0.2189\t\t1.0196\t\t\t0.8370\t\t0.21x\t\t0.26x\n",
            "256x256\t\t0.0238\t\t0.0167\t\t\t0.0171\t\t1.42x\t\t1.39x\n",
            "512x512\t\t0.3745\t\t0.3034\t\t\t0.3421\t\t1.23x\t\t1.09x\n",
            "1024x1024\t\t6.0037\t\t3.7350\t\t\t3.7350\t\t1.61x\t\t1.61x\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "from numba import njit, prange\n",
        "import time\n",
        "\n",
        "# Task 1: Simulate variable workload per iteration\n",
        "@njit\n",
        "def process_image_serial(image_sizes, results):\n",
        "    \"\"\"Serial processing with variable workload\"\"\"\n",
        "    n = len(image_sizes)\n",
        "    for i in range(n):\n",
        "        # Simulate variable computation based on image size\n",
        "        workload = image_sizes[i]\n",
        "        computation = 0.0\n",
        "        for j in range(workload):\n",
        "            computation += np.sqrt(j + 1) * np.sin(j * 0.1)\n",
        "        results[i] = computation\n",
        "    return results\n",
        "\n",
        "# Task 2: Parallelize using prange\n",
        "@njit(parallel=True)\n",
        "def process_image_parallel(image_sizes, results):\n",
        "    \"\"\"Parallel processing with variable workload\"\"\"\n",
        "    n = len(image_sizes)\n",
        "    for i in prange(n):\n",
        "        # Simulate variable computation based on image size\n",
        "        workload = image_sizes[i]\n",
        "        computation = 0.0\n",
        "        for j in range(workload):\n",
        "            computation += np.sqrt(j + 1) * np.sin(j * 0.1)\n",
        "        results[i] = computation\n",
        "    return results\n",
        "\n",
        "# Task 3: Observe execution time variation\n",
        "def benchmark_load_imbalance():\n",
        "    \"\"\"Analyze load imbalance effects\"\"\"\n",
        "    n_images = 1000\n",
        "\n",
        "    # Create different workload distributions\n",
        "    print(\"Workload Distribution\\tSerial Time(s)\\tParallel Time(s)\\tSpeedup\\t\\tEfficiency\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    # Uniform workload\n",
        "    uniform_sizes = np.full(n_images, 10000, dtype=np.int32)\n",
        "    results_serial = np.zeros(n_images, dtype=np.float64)\n",
        "    results_parallel = np.zeros(n_images, dtype=np.float64)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    process_image_serial(uniform_sizes, results_serial)\n",
        "    serial_time = time.perf_counter() - start\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    process_image_parallel(uniform_sizes, results_parallel)\n",
        "    parallel_time = time.perf_counter() - start\n",
        "\n",
        "    speedup = serial_time / parallel_time\n",
        "    efficiency = speedup / numba.config.NUMBA_NUM_THREADS * 100\n",
        "    print(f\"Uniform\\t\\t\\t{serial_time:.4f}\\t\\t{parallel_time:.4f}\\t\\t{speedup:.2f}x\\t\\t{efficiency:.1f}%\")\n",
        "\n",
        "    # Imbalanced workload (ascending)\n",
        "    imbalanced_sizes = np.linspace(1000, 20000, n_images, dtype=np.int32)\n",
        "    results_serial = np.zeros(n_images, dtype=np.float64)\n",
        "    results_parallel = np.zeros(n_images, dtype=np.float64)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    process_image_serial(imbalanced_sizes, results_serial)\n",
        "    serial_time = time.perf_counter() - start\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    process_image_parallel(imbalanced_sizes, results_parallel)\n",
        "    parallel_time = time.perf_counter() - start\n",
        "\n",
        "    speedup = serial_time / parallel_time\n",
        "    efficiency = speedup / numba.config.NUMBA_NUM_THREADS * 100\n",
        "    print(f\"Imbalanced (Ascending)\\t{serial_time:.4f}\\t\\t{parallel_time:.4f}\\t\\t{speedup:.2f}x\\t\\t{efficiency:.1f}%\")\n",
        "\n",
        "    # Random workload\n",
        "    np.random.seed(42)\n",
        "    random_sizes = np.random.randint(1000, 20000, n_images, dtype=np.int32)\n",
        "    results_serial = np.zeros(n_images, dtype=np.float64)\n",
        "    results_parallel = np.zeros(n_images, dtype=np.float64)\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    process_image_serial(random_sizes, results_serial)\n",
        "    serial_time = time.perf_counter() - start\n",
        "\n",
        "    start = time.perf_counter()\n",
        "    process_image_parallel(random_sizes, results_parallel)\n",
        "    parallel_time = time.perf_counter() - start\n",
        "\n",
        "    speedup = serial_time / parallel_time\n",
        "    efficiency = speedup / numba.config.NUMBA_NUM_THREADS * 100\n",
        "    print(f\"Random\\t\\t\\t{serial_time:.4f}\\t\\t{parallel_time:.4f}\\t\\t{speedup:.2f}x\\t\\t{efficiency:.1f}%\")\n",
        "\n",
        "# Task 4: Discussion on limitations\n",
        "def print_scheduling_limitations():\n",
        "    \"\"\"Discuss Python/Numba scheduling limitations\"\"\"\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"SCHEDULING LIMITATIONS IN PYTHON/NUMBA\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\n1. Static Scheduling:\")\n",
        "    print(\"   - Numba's prange uses static scheduling by default\")\n",
        "    print(\"   - Work is divided equally among threads at compile time\")\n",
        "    print(\"   - No runtime load balancing unlike OpenMP's dynamic/guided scheduling\")\n",
        "\n",
        "    print(\"\\n2. Load Imbalance Impact:\")\n",
        "    print(\"   - Threads with lighter work finish early and remain idle\")\n",
        "    print(\"   - Thread with heaviest work determines total execution time\")\n",
        "    print(\"   - Efficiency degrades with increasing workload variance\")\n",
        "\n",
        "    print(\"\\n3. OpenMP Comparison:\")\n",
        "    print(\"   - OpenMP offers: schedule(static), schedule(dynamic), schedule(guided)\")\n",
        "    print(\"   - Numba lacks equivalent scheduling control\")\n",
        "    print(\"   - No work-stealing or dynamic chunk assignment in Numba\")\n",
        "\n",
        "    print(\"\\n4. Workarounds:\")\n",
        "    print(\"   - Pre-sort tasks by workload (decreasing order)\")\n",
        "    print(\"   - Use task parallelism instead of data parallelism\")\n",
        "    print(\"   - Consider Dask or Ray for better load balancing\")\n",
        "    print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_load_imbalance()\n",
        "    print_scheduling_limitations()"
      ],
      "metadata": {
        "id": "huhcWASVDF3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "from numba import njit, prange\n",
        "import time\n",
        "\n",
        "# Task 1 & 2: Serial versions\n",
        "@njit\n",
        "def compute_sum_serial(data):\n",
        "    \"\"\"Serial sum computation\"\"\"\n",
        "    total = 0.0\n",
        "    for i in range(len(data)):\n",
        "        total += data[i]\n",
        "    return total\n",
        "\n",
        "@njit\n",
        "def compute_max_serial(data):\n",
        "    \"\"\"Serial maximum computation\"\"\"\n",
        "    max_val = data[0]\n",
        "    for i in range(1, len(data)):\n",
        "        if data[i] > max_val:\n",
        "            max_val = data[i]\n",
        "    return max_val\n",
        "\n",
        "# Task 2: Parallel versions\n",
        "@njit(parallel=True)\n",
        "def compute_sum_parallel(data):\n",
        "    \"\"\"Parallel sum using reduction\"\"\"\n",
        "    total = 0.0\n",
        "    for i in prange(len(data)):\n",
        "        total += data[i]\n",
        "    return total\n",
        "\n",
        "@njit(parallel=True)\n",
        "def compute_max_parallel(data):\n",
        "    \"\"\"Parallel maximum using reduction\"\"\"\n",
        "    n = len(data)\n",
        "    max_val = data[0]\n",
        "    for i in prange(n):\n",
        "        if data[i] > max_val:\n",
        "            max_val = data[i]\n",
        "    return max_val\n",
        "\n",
        "# Alternative: Manual reduction with local maxima\n",
        "@njit(parallel=True)\n",
        "def compute_sum_parallel_manual(data):\n",
        "    \"\"\"Manual parallel sum with thread-local accumulation\"\"\"\n",
        "    n = len(data)\n",
        "    num_threads = numba.get_num_threads()\n",
        "    chunk_size = (n + num_threads - 1) // num_threads\n",
        "    partial_sums = np.zeros(num_threads, dtype=np.float64)\n",
        "\n",
        "    for i in prange(n):\n",
        "        thread_id = i // chunk_size\n",
        "        if thread_id < num_threads:\n",
        "            partial_sums[thread_id] += data[i]\n",
        "\n",
        "    total = 0.0\n",
        "    for i in range(num_threads):\n",
        "        total += partial_sums[i]\n",
        "    return total\n",
        "\n",
        "# Task 3: Verify correctness and performance\n",
        "def benchmark_reductions():\n",
        "    \"\"\"Test correctness and measure performance\"\"\"\n",
        "    sizes = [10_000_000, 50_000_000, 100_000_000, 500_000_000]\n",
        "\n",
        "    print(\"=\"*90)\n",
        "    print(\"PARALLEL REDUCTION OPERATIONS - SUM\")\n",
        "    print(\"=\"*90)\n",
        "    print(\"Data Size\\tSerial Time(s)\\tParallel Time(s)\\tSpeedup\\t\\tCorrect\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    for size in sizes:\n",
        "        data = np.random.rand(size).astype(np.float64)\n",
        "\n",
        "        # Warm-up\n",
        "        compute_sum_serial(data[:1000])\n",
        "        compute_sum_parallel(data[:1000])\n",
        "\n",
        "        # Serial sum\n",
        "        start = time.perf_counter()\n",
        "        sum_serial = compute_sum_serial(data)\n",
        "        serial_time = time.perf_counter() - start\n",
        "\n",
        "        # Parallel sum\n",
        "        start = time.perf_counter()\n",
        "        sum_parallel = compute_sum_parallel(data)\n",
        "        parallel_time = time.perf_counter() - start\n",
        "\n",
        "        speedup = serial_time / parallel_time\n",
        "        correct = np.isclose(sum_serial, sum_parallel, rtol=1e-5)\n",
        "\n",
        "        print(f\"{size:>11,}\\t{serial_time:.4f}\\t\\t{parallel_time:.4f}\\t\\t\\t{speedup:.2f}x\\t\\t{correct}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*90)\n",
        "    print(\"PARALLEL REDUCTION OPERATIONS - MAXIMUM\")\n",
        "    print(\"=\"*90)\n",
        "    print(\"Data Size\\tSerial Time(s)\\tParallel Time(s)\\tSpeedup\\t\\tCorrect\")\n",
        "    print(\"-\" * 90)\n",
        "\n",
        "    for size in sizes:\n",
        "        data = np.random.rand(size).astype(np.float64)\n",
        "\n",
        "        # Warm-up\n",
        "        compute_max_serial(data[:1000])\n",
        "        compute_max_parallel(data[:1000])\n",
        "\n",
        "        # Serial max\n",
        "        start = time.perf_counter()\n",
        "        max_serial = compute_max_serial(data)\n",
        "        serial_time = time.perf_counter() - start\n",
        "\n",
        "        # Parallel max\n",
        "        start = time.perf_counter()\n",
        "        max_parallel = compute_max_parallel(data)\n",
        "        parallel_time = time.perf_counter() - start\n",
        "\n",
        "        speedup = serial_time / parallel_time\n",
        "        correct = np.isclose(max_serial, max_parallel)\n",
        "\n",
        "        print(f\"{size:>11,}\\t{serial_time:.4f}\\t\\t{parallel_time:.4f}\\t\\t\\t{speedup:.2f}x\\t\\t{correct}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_reductions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3M8Tz2ILDFlX",
        "outputId": "7cb967f9-b29f-48ca-db93-a342a55413c2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "PARALLEL REDUCTION OPERATIONS - SUM\n",
            "==========================================================================================\n",
            "Data Size\tSerial Time(s)\tParallel Time(s)\tSpeedup\t\tCorrect\n",
            "------------------------------------------------------------------------------------------\n",
            " 10,000,000\t0.0175\t\t0.0115\t\t\t1.52x\t\tTrue\n",
            " 50,000,000\t0.0837\t\t0.0463\t\t\t1.81x\t\tTrue\n",
            "100,000,000\t0.1725\t\t0.1069\t\t\t1.61x\t\tTrue\n",
            "500,000,000\t0.8376\t\t0.5052\t\t\t1.66x\t\tTrue\n",
            "\n",
            "==========================================================================================\n",
            "PARALLEL REDUCTION OPERATIONS - MAXIMUM\n",
            "==========================================================================================\n",
            "Data Size\tSerial Time(s)\tParallel Time(s)\tSpeedup\t\tCorrect\n",
            "------------------------------------------------------------------------------------------\n",
            " 10,000,000\t0.0175\t\t0.0001\t\t\t201.97x\t\tFalse\n",
            " 50,000,000\t0.0954\t\t0.0001\t\t\t1000.96x\t\tFalse\n",
            "100,000,000\t0.1891\t\t0.0001\t\t\t2153.12x\t\tFalse\n",
            "500,000,000\t0.8710\t\t0.0001\t\t\t12723.37x\t\tFalse\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import numba\n",
        "from numba import njit, prange\n",
        "import time\n",
        "\n",
        "# Task 1: Serial Monte Carlo simulation\n",
        "@njit\n",
        "def monte_carlo_pi_serial(n_samples):\n",
        "    \"\"\"Serial Monte Carlo estimation of pi\"\"\"\n",
        "    count_inside = 0\n",
        "\n",
        "    # Use Numba-compatible random number generation\n",
        "    np.random.seed(42)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        x = np.random.random()\n",
        "        y = np.random.random()\n",
        "\n",
        "        if x*x + y*y <= 1.0:\n",
        "            count_inside += 1\n",
        "\n",
        "    pi_estimate = 4.0 * count_inside / n_samples\n",
        "    return pi_estimate\n",
        "\n",
        "# Task 2: Parallel version using prange\n",
        "@njit(parallel=True)\n",
        "def monte_carlo_pi_parallel(n_samples):\n",
        "    \"\"\"Parallel Monte Carlo estimation of pi using prange\"\"\"\n",
        "    count_inside = 0\n",
        "\n",
        "    # Set seed for reproducibility (each thread will have different subsequence)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    for i in prange(n_samples):\n",
        "        x = np.random.random()\n",
        "        y = np.random.random()\n",
        "\n",
        "        if x*x + y*y <= 1.0:\n",
        "            count_inside += 1\n",
        "\n",
        "    pi_estimate = 4.0 * count_inside / n_samples\n",
        "    return pi_estimate\n",
        "\n",
        "# Alternative: Thread-safe version with per-thread RNG\n",
        "@njit(parallel=True)\n",
        "def monte_carlo_pi_parallel_safe(n_samples):\n",
        "    \"\"\"Parallel Monte Carlo with thread-local counting\"\"\"\n",
        "    num_threads = numba.get_num_threads()\n",
        "    counts = np.zeros(num_threads, dtype=np.int64)\n",
        "\n",
        "    for i in prange(n_samples):\n",
        "        # Generate random numbers\n",
        "        state = i + 12345  # Simple seed based on iteration\n",
        "        x = (state * 1103515245 + 12345) % 2147483648 / 2147483648.0\n",
        "        y = ((state + 1) * 1103515245 + 12345) % 2147483648 / 2147483648.0\n",
        "\n",
        "        if x*x + y*y <= 1.0:\n",
        "            thread_id = numba.get_thread_id()\n",
        "            counts[thread_id] += 1\n",
        "\n",
        "    total_inside = np.sum(counts)\n",
        "    pi_estimate = 4.0 * total_inside / n_samples\n",
        "    return pi_estimate\n",
        "\n",
        "# Task 3 & 4: Experiments and timing\n",
        "def benchmark_monte_carlo():\n",
        "    \"\"\"Measure execution time and speedup for different sample sizes\"\"\"\n",
        "    sample_sizes = [50_000_000, 100_000_000, 500_000_000, 1_000_000_000]\n",
        "\n",
        "    print(\"=\"*100)\n",
        "    print(\"MONTE CARLO SIMULATION FOR π ESTIMATION\")\n",
        "    print(\"=\"*100)\n",
        "    print(\"Samples\\t\\t\\tSerial Time(s)\\tParallel Time(s)\\tSpeedup\\t\\tPi Estimate\\t\\tError\")\n",
        "    print(\"-\" * 100)\n",
        "\n",
        "    for n_samples in sample_sizes:\n",
        "        # Warm-up\n",
        "        monte_carlo_pi_serial(1000)\n",
        "        monte_carlo_pi_parallel(1000)\n",
        "\n",
        "        # Serial timing\n",
        "        start = time.perf_counter()\n",
        "        pi_serial = monte_carlo_pi_serial(n_samples)\n",
        "        serial_time = time.perf_counter() - start\n",
        "\n",
        "        # Parallel timing\n",
        "        start = time.perf_counter()\n",
        "        pi_parallel = monte_carlo_pi_parallel(n_samples)\n",
        "        parallel_time = time.perf_counter() - start\n",
        "\n",
        "        speedup = serial_time / parallel_time\n",
        "        error = abs(pi_parallel - np.pi)\n",
        "\n",
        "        print(f\"{n_samples:>13,}\\t{serial_time:>8.4f}\\t{parallel_time:>10.4f}\\t\\t{speedup:>6.2f}x\\t\\t{pi_parallel:.6f}\\t\\t{error:.6f}\")\n",
        "\n",
        "# Task 5: Discussion on race conditions and reduction\n",
        "def discuss_race_conditions():\n",
        "    \"\"\"Discuss race conditions and reduction handling\"\"\"\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(\"RACE CONDITIONS AND REDUCTION HANDLING\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    print(\"\\n1. RACE CONDITION IN NAIVE PARALLEL IMPLEMENTATION:\")\n",
        "    print(\"   - Multiple threads increment 'count_inside' simultaneously\")\n",
        "    print(\"   - Without synchronization: lost updates, incorrect results\")\n",
        "    print(\"   - Example: Thread A reads count=100, Thread B reads count=100\")\n",
        "    print(\"             Both increment to 101, write back → one update lost!\")\n",
        "\n",
        "    print(\"\\n2. NUMBA'S AUTOMATIC REDUCTION:\")\n",
        "    print(\"   - Numba recognizes reduction patterns (+=, min, max)\")\n",
        "    print(\"   - Implements thread-local accumulators automatically\")\n",
        "    print(\"   - Combines results at the end with proper synchronization\")\n",
        "    print(\"   - Similar to OpenMP's 'reduction(+:count_inside)'\")\n",
        "\n",
        "    print(\"\\n3. REDUCTION MECHANISM:\")\n",
        "    print(\"   Step 1: Each thread maintains private counter\")\n",
        "    print(\"   Step 2: Threads count points independently (no contention)\")\n",
        "    print(\"   Step 3: Private counters combined with atomic operations\")\n",
        "    print(\"   Step 4: Final sum computed safely\")\n",
        "\n",
        "    print(\"\\n4. RANDOM NUMBER GENERATION CONSIDERATIONS:\")\n",
        "    print(\"   - np.random.random() in parallel loops: each thread gets subsequence\")\n",
        "    print(\"   - Potential correlation between threads\")\n",
        "    print(\"   - For production: use per-thread RNG with different seeds\")\n",
        "    print(\"   - Alternative: XORShift, PCG, or other parallel-friendly PRNGs\")\n",
        "\n",
        "    print(\"\\n5. PERFORMANCE FACTORS:\")\n",
        "    print(\"   - Reduction overhead: minimal for large n_samples\")\n",
        "    print(\"   - Memory bandwidth: each thread reads/writes independently\")\n",
        "    print(\"   - Cache effects: good locality within each thread\")\n",
        "    print(\"   - Scalability: near-linear up to memory bandwidth limit\")\n",
        "\n",
        "    print(\"\\n6. VERIFICATION:\")\n",
        "    n_samples = 100_000_000\n",
        "    pi_estimate = monte_carlo_pi_parallel(n_samples)\n",
        "    error = abs(pi_estimate - np.pi)\n",
        "    print(f\"   π estimate with {n_samples:,} samples: {pi_estimate:.8f}\")\n",
        "    print(f\"   Actual π value: {np.pi:.8f}\")\n",
        "    print(f\"   Absolute error: {error:.8f}\")\n",
        "    print(f\"   Relative error: {error/np.pi * 100:.4f}%\")\n",
        "\n",
        "    print(\"=\"*100 + \"\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    benchmark_monte_carlo()\n",
        "    discuss_race_conditions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2R5tG4VDFV0",
        "outputId": "fbf84d05-2e62-46e2-9051-0579dfd44508"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "====================================================================================================\n",
            "MONTE CARLO SIMULATION FOR π ESTIMATION\n",
            "====================================================================================================\n",
            "Samples\t\t\tSerial Time(s)\tParallel Time(s)\tSpeedup\t\tPi Estimate\t\tError\n",
            "----------------------------------------------------------------------------------------------------\n",
            "   50,000,000\t  0.6816\t    0.6036\t\t  1.13x\t\t3.141988\t\t0.000395\n",
            "  100,000,000\t  1.2716\t    1.2363\t\t  1.03x\t\t3.141567\t\t0.000026\n",
            "  500,000,000\t  7.6370\t    6.1137\t\t  1.25x\t\t3.141622\t\t0.000029\n",
            "1,000,000,000\t 14.7445\t   13.2960\t\t  1.11x\t\t3.141542\t\t0.000051\n",
            "\n",
            "====================================================================================================\n",
            "RACE CONDITIONS AND REDUCTION HANDLING\n",
            "====================================================================================================\n",
            "\n",
            "1. RACE CONDITION IN NAIVE PARALLEL IMPLEMENTATION:\n",
            "   - Multiple threads increment 'count_inside' simultaneously\n",
            "   - Without synchronization: lost updates, incorrect results\n",
            "   - Example: Thread A reads count=100, Thread B reads count=100\n",
            "             Both increment to 101, write back → one update lost!\n",
            "\n",
            "2. NUMBA'S AUTOMATIC REDUCTION:\n",
            "   - Numba recognizes reduction patterns (+=, min, max)\n",
            "   - Implements thread-local accumulators automatically\n",
            "   - Combines results at the end with proper synchronization\n",
            "   - Similar to OpenMP's 'reduction(+:count_inside)'\n",
            "\n",
            "3. REDUCTION MECHANISM:\n",
            "   Step 1: Each thread maintains private counter\n",
            "   Step 2: Threads count points independently (no contention)\n",
            "   Step 3: Private counters combined with atomic operations\n",
            "   Step 4: Final sum computed safely\n",
            "\n",
            "4. RANDOM NUMBER GENERATION CONSIDERATIONS:\n",
            "   - np.random.random() in parallel loops: each thread gets subsequence\n",
            "   - Potential correlation between threads\n",
            "   - For production: use per-thread RNG with different seeds\n",
            "   - Alternative: XORShift, PCG, or other parallel-friendly PRNGs\n",
            "\n",
            "5. PERFORMANCE FACTORS:\n",
            "   - Reduction overhead: minimal for large n_samples\n",
            "   - Memory bandwidth: each thread reads/writes independently\n",
            "   - Cache effects: good locality within each thread\n",
            "   - Scalability: near-linear up to memory bandwidth limit\n",
            "\n",
            "6. VERIFICATION:\n",
            "   π estimate with 100,000,000 samples: 3.14192260\n",
            "   Actual π value: 3.14159265\n",
            "   Absolute error: 0.00032995\n",
            "   Relative error: 0.0105%\n",
            "====================================================================================================\n",
            "\n"
          ]
        }
      ]
    }
  ]
}